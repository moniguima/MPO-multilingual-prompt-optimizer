{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Prompt Optimizer - Interactive Demo\n",
    "\n",
    "This notebook demonstrates the **Multilingual Prompt Optimizer** (MPO) - a system that adapts LLM prompts for cultural and linguistic appropriateness across languages.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "1. How cultural adaptation improves LLM outputs\n",
    "2. The difference between translation and cultural transformation\n",
    "3. Comparing German (low-context) vs. Spanish (high-context) adaptations\n",
    "4. Metrics for evaluating multilingual prompt quality\n",
    "\n",
    "## üöÄ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from mpo.core.prompt import PromptTemplate, PromptDomain, FormalityLevel\n",
    "from mpo.adapters import get_adapter, EnglishAdapter, GermanAdapter, SpanishAdapter\n",
    "from mpo.providers import LocalLLMProvider\n",
    "from mpo.core.evaluator import PromptEvaluator\n",
    "from mpo.storage.cache_manager import CacheManager\n",
    "from mpo.metrics import quantitative, qualitative\n",
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Part 1: Understanding Cultural Adaptation\n",
    "\n",
    "### The Problem: Translation ‚â† Cultural Appropriateness\n",
    "\n",
    "Consider a simple business request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English baseline prompt\n",
    "english_prompt = \"\"\"\n",
    "I need to request an extension for the project deadline.\n",
    "The current deadline is next Friday, but I need until next month.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üá∫üá∏ English (baseline):\")\n",
    "print(english_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Translation vs. Cultural Adaptation\n",
    "\n",
    "**Naive Translation** (word-for-word):\n",
    "- üá©üá™ German: \"Ich brauche eine Verl√§ngerung f√ºr die Projektfrist...\"\n",
    "- üá™üá∏ Spanish: \"Necesito solicitar una extensi√≥n para la fecha l√≠mite...\"\n",
    "\n",
    "**Problem**: While semantically correct, these translations ignore:\n",
    "- ‚ùå Cultural communication norms (direct vs. indirect)\n",
    "- ‚ùå Formality expectations (Sie vs. du, usted vs. t√∫)\n",
    "- ‚ùå Relationship dynamics (task-focused vs. relational)\n",
    "\n",
    "**Our Approach**: Apply cultural transformation rules based on linguistic theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç Part 2: Loading Configuration\n",
    "\n",
    "Our system uses YAML configuration files with cultural parameters for each language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load language configurations\n",
    "with open('../config/languages.yaml') as f:\n",
    "    languages_config = yaml.safe_load(f)\n",
    "\n",
    "# Inspect German cultural parameters\n",
    "print(\"üá©üá™ German Cultural Parameters:\")\n",
    "print(f\"Context Level: {languages_config['languages']['de']['cultural_params']['communication_style']['context_level']}\")\n",
    "print(f\"Directness: {languages_config['languages']['de']['cultural_params']['communication_style']['directness']}\")\n",
    "print(f\"\\nFormality Markers (Formal):\")\n",
    "print(languages_config['languages']['de']['cultural_params']['formality_levels']['formal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Spanish cultural parameters\n",
    "print(\"üá™üá∏ Spanish Cultural Parameters:\")\n",
    "print(f\"Context Level: {languages_config['languages']['es']['cultural_params']['communication_style']['context_level']}\")\n",
    "print(f\"Directness: {languages_config['languages']['es']['cultural_params']['communication_style']['directness']}\")\n",
    "print(f\"\\nFormality Markers (Formal):\")\n",
    "print(languages_config['languages']['es']['cultural_params']['formality_levels']['formal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Part 3: Cultural Adaptation in Action\n",
    "\n",
    "Let's create a prompt template and adapt it for different languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "template = PromptTemplate(\n",
    "    id=\"business_request\",\n",
    "    content=\"I need to request an extension for the {project_name} project. The current deadline is {current_deadline}, but due to {reason}, I would like to request moving the deadline to {requested_deadline}. Could you please consider this request and let me know if this adjustment is possible?\",\n",
    "    domain=PromptDomain.BUSINESS,\n",
    "    placeholders={\n",
    "        \"project_name\": \"Website Redesign\",\n",
    "        \"current_deadline\": \"Friday, Oct 15\",\n",
    "        \"reason\": \"additional client requirements\",\n",
    "        \"requested_deadline\": \"Friday, Oct 29\"\n",
    "    },\n",
    "    description=\"Business email requesting project deadline extension\"\n",
    ")\n",
    "\n",
    "print(\"üìÑ Original Template:\")\n",
    "print(template.content)\n",
    "print(f\"\\nDomain: {template.domain.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation 1: German (Formal)\n",
    "\n",
    "**Cultural Context:**\n",
    "- **Low-context culture**: Information must be explicit\n",
    "- **High directness**: Get to the point quickly\n",
    "- **Formal pronouns**: Use \"Sie\" in business\n",
    "- **Structured**: Clear opening, body, closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt for German (formal)\n",
    "de_adapter = get_adapter('de', languages_config['languages']['de'])\n",
    "de_variant = de_adapter.adapt(template, FormalityLevel.FORMAL)\n",
    "\n",
    "print(\"üá©üá™ German Formal Adaptation:\")\n",
    "print(\"=\"*60)\n",
    "print(de_variant.adapted_content)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìã Adaptation Notes:\")\n",
    "print(de_variant.adaptation_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation 2: Spanish (Formal)\n",
    "\n",
    "**Cultural Context:**\n",
    "- **High-context culture**: Relationship matters\n",
    "- **Medium directness**: Balance task and relationship\n",
    "- **Formal pronouns**: Use \"usted\" in business\n",
    "- **Relational preambles**: Well-being inquiry + purpose statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt for Spanish (formal)\n",
    "es_adapter = get_adapter('es', languages_config['languages']['es'])\n",
    "es_variant = es_adapter.adapt(template, FormalityLevel.FORMAL)\n",
    "\n",
    "print(\"üá™üá∏ Spanish Formal Adaptation:\")\n",
    "print(\"=\"*60)\n",
    "print(es_variant.adapted_content)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìã Adaptation Notes:\")\n",
    "print(es_variant.adaptation_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Key Differences\n",
    "\n",
    "Compare the two adaptations:\n",
    "\n",
    "| Aspect | German (DE) | Spanish (ES) |\n",
    "|--------|-------------|-------------|\n",
    "| **Opening** | Direct greeting | Well-being inquiry |\n",
    "| **Preamble** | Brief context | Relational connection |\n",
    "| **Body** | Task-focused | Task + relationship |\n",
    "| **Closing** | Standard formal | Gratitude + formal |\n",
    "| **Tone** | Professional directness | Warm professionalism |\n",
    "\n",
    "This demonstrates **pragmatic equivalence** over **semantic equivalence**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Part 4: Generating LLM Responses (Demo Mode)\n",
    "\n",
    "Now let's retrieve cached LLM responses generated with Gemma 2 9B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cache manager\n",
    "cache = CacheManager('../data/cache')\n",
    "\n",
    "# Retrieve cached responses\n",
    "de_response = cache.get_cached_response('business_email', 'de', 'formal')\n",
    "es_response = cache.get_cached_response('business_email', 'es', 'formal')\n",
    "en_response = cache.get_cached_response('business_email', 'en', 'formal')\n",
    "\n",
    "print(\"‚úÖ Cached responses loaded\")\n",
    "print(f\"English: {len(en_response.content) if en_response else 0} chars\")\n",
    "print(f\"German: {len(de_response.content) if de_response else 0} chars\")\n",
    "print(f\"Spanish: {len(es_response.content) if es_response else 0} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display German response\n",
    "if de_response:\n",
    "    print(\"üá©üá™ German LLM Response:\")\n",
    "    print(\"=\"*60)\n",
    "    print(de_response.content)\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Tokens: {de_response.tokens_input} in / {de_response.tokens_output} out\")\n",
    "    print(f\"Model: {de_response.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Spanish response\n",
    "if es_response:\n",
    "    print(\"üá™üá∏ Spanish LLM Response:\")\n",
    "    print(\"=\"*60)\n",
    "    print(es_response.content)\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Tokens: {es_response.tokens_input} in / {es_response.tokens_output} out\")\n",
    "    print(f\"Model: {es_response.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 5: Metrics and Evaluation\n",
    "\n",
    "Let's calculate quantitative and qualitative metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for German response\n",
    "if de_response:\n",
    "    de_quant = quantitative.calculate_all_quantitative_metrics(\n",
    "        de_response.content,\n",
    "        de_response.tokens_output,\n",
    "        'de'\n",
    "    )\n",
    "    \n",
    "    de_qual = qualitative.calculate_all_qualitative_metrics(\n",
    "        de_response.content,\n",
    "        'de',\n",
    "        'formal',\n",
    "        'business'\n",
    "    )\n",
    "    \n",
    "    print(\"üá©üá™ German Metrics:\")\n",
    "    print(f\"  Word Count: {de_quant['length_metrics']['word_count']}\")\n",
    "    print(f\"  Lexical Diversity: {de_quant['lexical_diversity']['type_token_ratio']:.3f}\")\n",
    "    print(f\"  Avg Word Length: {de_quant['length_metrics']['avg_word_length']:.2f}\")\n",
    "    print(f\"  Cultural Appropriateness: {de_qual['cultural_appropriateness']['overall_rating']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for Spanish response\n",
    "if es_response:\n",
    "    es_quant = quantitative.calculate_all_quantitative_metrics(\n",
    "        es_response.content,\n",
    "        es_response.tokens_output,\n",
    "        'es'\n",
    "    )\n",
    "    \n",
    "    es_qual = qualitative.calculate_all_qualitative_metrics(\n",
    "        es_response.content,\n",
    "        'es',\n",
    "        'formal',\n",
    "        'business'\n",
    "    )\n",
    "    \n",
    "    print(\"üá™üá∏ Spanish Metrics:\")\n",
    "    print(f\"  Word Count: {es_quant['length_metrics']['word_count']}\")\n",
    "    print(f\"  Lexical Diversity: {es_quant['lexical_diversity']['type_token_ratio']:.3f}\")\n",
    "    print(f\"  Avg Word Length: {es_quant['length_metrics']['avg_word_length']:.2f}\")\n",
    "    print(f\"  Cultural Appropriateness: {es_qual['cultural_appropriateness']['overall_rating']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Part 6: Visualization\n",
    "\n",
    "Let's create a simple comparison chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Prepare data\n",
    "languages = ['English', 'German', 'Spanish']\n",
    "responses = [en_response, de_response, es_response]\n",
    "\n",
    "token_counts = [r.tokens_output if r else 0 for r in responses]\n",
    "word_counts = [\n",
    "    quantitative.calculate_all_quantitative_metrics(r.content, r.tokens_output, lang)['length_metrics']['word_count']\n",
    "    if r else 0\n",
    "    for r, lang in zip(responses, ['en', 'de', 'es'])\n",
    "]\n",
    "\n",
    "# Create bar chart\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Tokens', x=languages, y=token_counts),\n",
    "    go.Bar(name='Words', x=languages, y=word_counts)\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Response Length Comparison',\n",
    "    barmode='group',\n",
    "    yaxis_title='Count',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 7: Key Takeaways\n",
    "\n",
    "### What We've Demonstrated:\n",
    "\n",
    "1. **Cultural Adaptation ‚â† Translation**\n",
    "   - German: Direct, structured, task-focused\n",
    "   - Spanish: Relational, warm, context-rich\n",
    "\n",
    "2. **Linguistic Theory in Practice**\n",
    "   - Hall's high/low-context framework\n",
    "   - Brown & Levinson's politeness theory\n",
    "   - T-V distinction (formal pronouns)\n",
    "\n",
    "3. **Measurable Quality Metrics**\n",
    "   - Lexical diversity\n",
    "   - Token efficiency\n",
    "   - Cultural appropriateness\n",
    "\n",
    "4. **Zero-Cost Local Inference**\n",
    "   - Gemma 2 9B provides excellent multilingual quality\n",
    "   - No API costs during development\n",
    "   - Real-time adaptation testing\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- üåç International business communication\n",
    "- ü§ñ Culturally-aware chatbots\n",
    "- üìß Automated email generation\n",
    "- üéì Language learning tools\n",
    "- üî¨ Cross-cultural NLP research\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Try experimenting with:\n",
    "1. Different formality levels (casual, neutral, formal)\n",
    "2. Other prompt templates (technical, creative, persuasive)\n",
    "3. Adding new languages (French, Japanese, etc.)\n",
    "4. Custom cultural parameters\n",
    "\n",
    "**CLI Commands:**\n",
    "```bash\n",
    "# Test different prompts\n",
    "mpo test business_email --provider local --live -l de -f formal\n",
    "\n",
    "# Generate HTML report\n",
    "mpo html-report business_email\n",
    "\n",
    "# Run full benchmark\n",
    "mpo benchmark --provider local\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Learn More:**\n",
    "- Read `docs/cultural_rationale.md` for linguistic theory details\n",
    "- Check `docs/architecture.md` for system design\n",
    "- See `GEMMA_2_9B_RESULTS.md` for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### üéØ Provider Comparison Summary\n\n| Feature | Local (Gemma 2) | OpenAI (GPT-4) |\n|---------|----------------|----------------|\n| **Quality** | Excellent | Outstanding |\n| **Speed** | Fast (local) | Medium (API) |\n| **Cost** | Free | ~$0.01-0.03/request |\n| **Embeddings** | ‚ùå Not available | ‚úÖ Native (1536-dim) |\n| **Token Counting** | Approximation (~4 chars/token) | Exact (tiktoken) |\n| **Languages** | Good multilingual | Excellent multilingual |\n| **Context Window** | 8K tokens | 128K tokens |\n| **Privacy** | ‚úÖ Fully local | ‚ö†Ô∏è Sent to OpenAI |\n| **Setup** | Requires LMStudio | API key only |\n\n### üí° Recommendation:\n\n- **Development & Testing**: Use Local provider (Gemma 2 9B)\n  - No costs\n  - Fast iteration\n  - Good quality for testing\n\n- **Production & Research**: Use OpenAI or Anthropic\n  - Higher quality\n  - Native embeddings (OpenAI)\n  - Better multilingual support\n  - Worth the cost for important applications\n\n### üöÄ Using OpenAI in Production:\n\n```python\n# Production setup\nfrom mpo.providers import OpenAIProvider\nfrom mpo.core.evaluator import PromptEvaluator\n\n# Initialize\nprovider = OpenAIProvider(api_key=\"sk-...\")\nevaluator = PromptEvaluator(provider, lang_config)\n\n# Generate\nresponse = evaluator.evaluate_variant(variant, config)\n\n# Get embeddings for RAG\nembeddings = provider.get_embeddings(text)\n```\n\n**CLI Usage:**\n```bash\n# Test with OpenAI\nmpo test business_email --provider openai --live -l de -f formal\n\n# Run benchmark with OpenAI\nmpo benchmark --provider openai --live\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compare token counting methods\ntest_text = \"I need to request an extension for the Website Redesign project.\"\n\n# OpenAI (accurate with tiktoken)\nopenai_tokens = openai_provider.count_tokens(test_text)\n\n# Local provider (approximation)\nlocal_provider = LocalLLMProvider()\nlocal_tokens = local_provider.count_tokens(test_text)\n\nprint(\"üî¢ Token Counting Comparison:\")\nprint(\"=\"*40)\nprint(f\"Text: \\\"{test_text}\\\"\")\nprint(f\"\\nOpenAI (tiktoken):   {openai_tokens} tokens\")\nprint(f\"Local (approximation): {local_tokens} tokens\")\nprint(f\"Character count:       {len(test_text)} chars\")\nprint(f\"\\nüí° OpenAI's tiktoken provides exact counts\")\nprint(f\"   Local uses ~4 chars/token approximation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üìä Token Counting Accuracy\n\nOpenAI provider uses **tiktoken** for accurate token counting, while local provider uses approximation:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Get embeddings for our prompts\nimport numpy as np\n\n# Sample texts in different languages\ntexts = {\n    'en': \"I need to request a project deadline extension.\",\n    'de': \"Ich m√∂chte h√∂flich um eine Fristverl√§ngerung bitten.\",\n    'es': \"Me dirijo a usted para solicitar una pr√≥rroga del plazo.\"\n}\n\nprint(\"üéØ Generating Embeddings...\")\nembeddings = {}\n\nfor lang, text in texts.items():\n    emb = openai_provider.get_embeddings(text)\n    embeddings[lang] = emb\n    print(f\"‚úÖ {lang.upper()}: {len(emb)}-dimensional vector\")\n\n# Calculate semantic similarity (cosine similarity)\ndef cosine_similarity(v1, v2):\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\nprint(\"\\nüìä Semantic Similarity Matrix:\")\nprint(\"-\" * 40)\nfor lang1 in texts.keys():\n    for lang2 in texts.keys():\n        if lang1 <= lang2:  # Avoid duplicates\n            sim = cosine_similarity(embeddings[lang1], embeddings[lang2])\n            print(f\"{lang1.upper()} ‚Üî {lang2.upper()}: {sim:.4f}\")\n\nprint(\"\\nüí° Interpretation:\")\nprint(\"   Values close to 1.0 = high semantic similarity\")\nprint(\"   All three express the same intent ‚Üí high similarity expected\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üéØ OpenAI-Specific Feature: Native Embeddings\n\nOne unique advantage of OpenAI provider is **native embeddings support** using `text-embedding-3-small` (1536 dimensions).\n\nThis enables:\n- Semantic similarity comparison\n- Document clustering\n- Retrieval-augmented generation (RAG)\n- Cross-lingual similarity matching\n\nLet's demonstrate embeddings:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Use the German formal variant we created earlier\nfrom mpo.providers.base import GenerationConfig\n\n# Create evaluator with OpenAI provider\nopenai_evaluator = PromptEvaluator(openai_provider, languages_config['languages'])\n\n# Generate response (only if you have API key and want to spend ~$0.02)\n# For demo purposes, we'll use the mock provider by default\n\nprint(\"üîÑ Generating response with OpenAI provider...\")\nprint(f\"   Language: German (de)\")\nprint(f\"   Formality: Formal\")\nprint(f\"   Provider: {openai_provider.provider_name}\\n\")\n\n# Configuration for generation\nconfig = GenerationConfig(\n    temperature=0.7,\n    max_tokens=500,\n    top_p=0.95\n)\n\n# Generate response\nopenai_response = openai_evaluator.evaluate_variant(de_variant, config)\n\nprint(\"‚úÖ Response generated!\")\nprint(f\"üìä Tokens: {openai_response.tokens_input} in / {openai_response.tokens_output} out\")\nprint(f\"‚è±Ô∏è  Timestamp: {openai_response.timestamp}\")\nprint(f\"\\nü§ñ GPT-4 Response:\")\nprint(\"=\"*60)\nprint(openai_response.content)\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Generating a Response with OpenAI\n\nLet's generate a German formal business email using GPT-4:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import OpenAI provider\nfrom mpo.providers import OpenAIProvider, MockOpenAIProvider\nimport os\n\n# Check if API key is available\nhas_openai_key = os.getenv(\"OPENAI_API_KEY\") is not None\n\nif has_openai_key:\n    print(\"‚úÖ OPENAI_API_KEY found - will use live OpenAI API\")\n    print(\"‚ö†Ô∏è  Note: This will make real API calls and cost ~$0.02\")\n    \n    # Initialize OpenAI provider\n    openai_provider = OpenAIProvider()\n    print(f\"ü§ñ Provider: {openai_provider.provider_name}\")\n    print(f\"üì¶ Model: {openai_provider.model_name}\")\n    \n    # Show model info\n    model_info = openai_provider.get_model_info()\n    print(f\"üí∞ Cost: ${model_info['cost_input_per_m']}/M input, ${model_info['cost_output_per_m']}/M output tokens\")\nelse:\n    print(\"‚ö†Ô∏è  OPENAI_API_KEY not found - using Mock provider for demo\")\n    print(\"üí° To use real OpenAI API: Add OPENAI_API_KEY to your .env file\")\n    \n    # Use mock provider for demonstration\n    openai_provider = MockOpenAIProvider()\n    print(f\"üé≠ Using Mock OpenAI Provider\")\n    print(f\"üì¶ Model: {openai_provider.model_name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ü§ñ Part 8: Using OpenAI Provider (GPT-4)\n\n### Why Use OpenAI Provider?\n\nWhile the local provider (Gemma 2 9B) is excellent for development and testing, the **OpenAI provider** offers:\n\n- ‚úÖ **Superior quality**: GPT-4 Turbo with advanced reasoning\n- ‚úÖ **Native embeddings**: Built-in semantic similarity (text-embedding-3-small)\n- ‚úÖ **Accurate token counting**: Using tiktoken library\n- ‚úÖ **Wider language support**: Better multilingual capabilities\n- ‚ö†Ô∏è **Costs money**: ~$0.01-0.03 per request\n\n### When to Use Each Provider:\n\n| Provider | Best For | Cost | Quality |\n|----------|----------|------|---------|\n| **Local (Gemma 2)** | Development, testing, demos | Free | Excellent |\n| **OpenAI (GPT-4)** | Production, research, high-stakes | Paid | Outstanding |\n| **Anthropic (Claude)** | Long contexts, nuanced writing | Paid | Outstanding |\n\nLet's demonstrate the OpenAI provider capabilities:",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}